---
title: "代码解析：Deep Graph Matching Consensus - dbp15k"
date: 2020-10-22T17:04:40+08:00
categories:
- 代码解析
katex: true
---

# Abstract
这篇博客是论文《Deep Graph Matching Consensus》 这篇论文的代码分析。
这篇论文来自，ICML2020，作者是 *Matthias Fey* ，这是写了 Pytorch Geomatric的大神，因此拜读下他的代码。
之前也研究过这个代码，但是没能看懂源码，这次是mou足了劲一定要看懂。
从2020/10/22上午开始一直到刚刚（2020-10-22 17:09:45）才算是完整的理解了这篇论文的代码，总共花费了18P。

# 思考和总结
1. 这个代码运行的超级快，基本上5分钟左右就可以迭代到非常好的结果
2. 代码写的很散，很随意，我自己迭代模型的时候也要注意，不需要在“辞藻”“格式”上太下功夫
3. 代码里有很多和论文中介绍不太一致的部分，需要仔细推敲
4. 代码里可以学到很多技巧关于pytorch，里面很多可以打包到我自己的工具包里的，注意采用
5. 我试了用手抄代码和写一个玩具模型的方式来尝试理解代码，后来总结感觉，直接创建一个玩具模型是最高效的，同时要做记录，用笔和纸记下所思所想，画下流程等等。

# 数据集
首先介绍下，我比较关注的实体对齐任务，数据集采用DBP15K。在我之前REGCN的实验中该数据集（来源于RDGCN）在zh-en分类下，source KG和target KG是合在一起的，总共有38960个entity. 而DGMC分开的，source KG有19388个entity，target KG有19572个entity. （合在一起确实是38960）。 <font color="red">这两个数据集的实体特征不知道用的是不是一样的</font>。但是有监督数据，RDGCN有15000个，而DGMC的训练集和测试集是分开的，train_y 有3205个，test_y有8788个，如果把RDGCN的训练集和测试集分开，则，训练集4500个，测试集10500个，DGMC的训练: 测试 = 0.36: 1，RDGCN的训练: 测试 = 0.42: 1。 <font color="red">训练集和测试集的比例不同</font>，不知道DGMC是怎么清洗的，这个划分比例应该效果更差点才对。

# Methodology

首先有2个GNN的模块：
```python
psi_1 = RelGNN(input: 300 -->  output: 256)
psi_2 = RelGNN(input: 32 -->  output: 32)
```
主模块为 DGMC()  
 stepin DGMC.forward()的代码

```python
h_s = self.psi_1(x_s, edge_index_s, edge_attr_s)
h_t = self.psi_1(x_t, edge_index_t, edge_attr_t)
```
先通过psi_1分别学习source KG 和 target KG中entity的embedding.
得到h_s和h_t，维度为 19388 x 256 和 19572 x 256.

然后获取h_s和h_t的**soft correspondences**, 在论文中表现为
$$
S^{(0)} = sinkhorn(\hat{S}^{(0)}) \in [0, 1]^{|v_s|\times |v_t|}\ with\ \hat{S}^{(0)} = H_s H_t^\intercal \in \mathbb{R}^{|v_s|\times |v_t|}
$$

```python
# ------ Sparse variant ------ #
S_idx = self.__top_k__(h_s, h_t)  # [B, N_s, k]
```
这里的`__top_k__`如下所示：
```python
def __top_k__(self, x_s, x_t):  # pragma: no cover
    r"""Memory-efficient top-k correspondence computation."""
    if LazyTensor is not None:
        x_s = x_s.unsqueeze(-2)  # [..., n_s, 1, d]
        x_t = x_t.unsqueeze(-3)  # [..., 1, n_t, d]
        x_s, x_t = LazyTensor(x_s), LazyTensor(x_t)
        S_ij = (-x_s * x_t).sum(dim=-1)
        return S_ij.argKmin(self.k, dim=2, backend=self.backend)
    else:
        x_s = x_s  # [..., n_s, d]
        x_t = x_t.transpose(-1, -2)  # [..., d, n_t]
        S_ij = x_s @ x_t
        return S_ij.topk(self.k, dim=2)[1]
```
即可以用KeOps包来快速求矩阵乘法，如果不用KeOps则可以用朴素的`else`中的代码。
这里有个有趣的东西，第一次知道Pytorch里用`@`可以直接表示矩阵乘法。  
而`torch.topk`表示取指定维度中最大的k个元素，返回两个值，第一个值为最大的k个元素的values，而第二个值为最大的k个元素的索引。

下面这个模块负责一个任务，注释里也写了，即除了“收集”topK个元素之外，还要随机采样k个元素，并且还要进行检查，如果这2k个元素里没有ground_truth，则在最后一维加上ground_truth.
```python
# In addition to the top-k, randomly sample negative examples and
# ensure that the ground-truth is included as a sparse entry.
if self.training and y is not None:
    rnd_size = (B, N_s, min(self.k, N_t - self.k))
    S_rnd_idx = torch.randint(N_t, rnd_size, dtype=torch.long,
                                device=S_idx.device)
    S_idx = torch.cat([S_idx, S_rnd_idx], dim=-1)
    S_idx = self.__include_gt__(S_idx, s_mask, y)
```
这样的话会得到一个新的`S_idx` 维度为：(1 x 19388 x 20), 这里的20就是新的k，既包含topK和random samples又包含ground_truth.  
继续往下看：
```python
k = S_idx.size(-1)
tmp_s = h_s.view(B, N_s, 1, C_out)
idx = S_idx.view(B, N_s * k, 1).expand(-1, -1, C_out)
tmp_t = torch.gather(h_t.view(B, N_t, C_out), -2, idx)
```
`idx` 就是将 `S_idx` 展开，维度为（1 x 387760 x 256）
可以看一眼，`idx` 如下：（387760 = 19388 x 20）
```python
tensor([[[16492, 16492, 16492,  ..., 16492, 16492, 16492],
         [12231, 12231, 12231,  ..., 12231, 12231, 12231],
         [ 2364,  2364,  2364,  ...,  2364,  2364,  2364],
         ...,
         [13908, 13908, 13908,  ..., 13908, 13908, 13908],
         [16987, 16987, 16987,  ..., 16987, 16987, 16987],
         [12498, 12498, 12498,  ..., 12498, 12498, 12498]]], device='cuda:0')
```
而`S_idx`如下：
```python
tensor([[[16492, 12231,  2364,  ...,  8276, 12392,     0],
         [17302,  1189, 14384,  ..., 10488,  5975,     1],
         [14384,  7479, 12856,  ...,  2019, 11954,     2],
         ...,
         [14384,  2581, 12856,  ...,  9599, 15871,  3933],
         [14384, 11952, 12856,  ..., 18672,  3663, 12765],
         [16527,  9158, 14656,  ..., 13908, 16987, 12498]]], device='cuda:0')
```
将`S_idx`展开，同时每个元素重复256遍。  
`torch.gather` （关于这个函数的使用参考：[传送门](https://zhuanlan.zhihu.com/p/101896024)）会返回一个和`idx` 的size一模一样的tensor，可以将387760个entity在h_t中的embedding取出来。这里模块的设计和我平常用的很不一样，我比较习惯于用torch.index_select，不知道这两者效率差异有多少。

然后就可以计算新的**correspondences matrix**：
```python
S_hat = (tmp_s * tmp_t.view(B, N_s, k, C_out)).sum(dim=-1)
S_0 = S_hat.softmax(dim=-1)[s_mask]
```
`tmp_s`的维度是 (1, 19388, 1, 256)  
`tmp_t.view(B, N_s, k, C_out)` 为 (1, 19388, 20,  256)  
其实得到的是所有的source KG中的实体和`S_idx`中sample出来的examples之间的相似度（类比于$\sqrt{x \cdot y}$ 欧氏距离）  

后面的部分就相当于将这种相似度转化为稀疏矩阵的形式
```python
 S_L = S_hat.softmax(dim=-1)[s_mask]
S_idx = S_idx[s_mask]

# Convert sparse layout to `torch.sparse_coo_tensor`.
row = torch.arange(x_s.size(0), device=S_idx.device)
row = row.view(-1, 1).repeat(1, k)
idx = torch.stack([row.view(-1), S_idx.view(-1)], dim=0)
size = torch.Size([x_s.size(0), N_t])

S_sparse_0 = torch.sparse_coo_tensor(
    idx, S_0.view(-1), size, requires_grad=S_0.requires_grad)
S_sparse_0.__idx__ = S_idx
S_sparse_0.__val__ = S_0

S_sparse_L = torch.sparse_coo_tensor(
    idx, S_L.view(-1), size, requires_grad=S_L.requires_grad)
S_sparse_L.__idx__ = S_idx
S_sparse_L.__val__ = S_L
```
以上属于**local feature matching**的部分，可以得到一个还不错的**correspondences matrix**来确定哪些元素之间是对齐的。实验结果为0.68左右。

这篇论文的第二个部分在于，**Refine correspondence matrix**. 由于第一部分是对邻域特征进行aggregate，而这种聚合主要是对邻居特征进行聚合，没有考虑纯粹的网络结构特征。用作者的话说就是 "Due to the purely local nature of the used node embeddings, our feature matching procedure is prone to finding false correspondences which are locally similar to the correct one. Formally, those cases pose a violation of the neighborhood consensus criteria employed in Equation " 

$$
\mathop{argmax}\limits_{\textbf{S}}\sum_{i,i^{\prime}\in V_s; j,j^{\prime}\in V_t} A_{i,i′}^{(s)} A_{j,j′}^{(t)} S_{i,j}S_{i′,j′}
$$

所以有了以下的代码：
```python
for _ in range(self.num_steps):
    S = S_hat.softmax(dim=-1)
    r_s = torch.randn((B, N_s, R_in), dtype=h_s.dtype,
                        device=h_s.device)

    tmp_t = r_s.view(B, N_s, 1, R_in) * S.view(B, N_s, k, 1)
    tmp_t = tmp_t.view(B, N_s * k, R_in)
    idx = S_idx.view(B, N_s * k, 1)
    r_t = scatter_add(tmp_t, idx, dim=1, dim_size=N_t)

    r_s, r_t = to_sparse(r_s, s_mask), to_sparse(r_t, t_mask)
    o_s = self.psi_2(r_s, edge_index_s, edge_attr_s)
    o_t = self.psi_2(r_t, edge_index_t, edge_attr_t)
    o_s, o_t = to_dense(o_s, s_mask), to_dense(o_t, t_mask)

    o_s = o_s.view(B, N_s, 1, R_out).expand(-1, -1, k, -1)
    idx = S_idx.view(B, N_s * k, 1).expand(-1, -1, R_out)
    tmp_t = torch.gather(o_t.view(B, N_t, R_out), -2, idx)
    D = o_s - tmp_t.view(B, N_s, k, R_out)
    S_hat = S_hat + self.mlp(D).squeeze(-1)
```
`num_steps` 就是论文中的 `L` 大小，代码中为10，且只在100次常规epoch之后进行了一批为10次的Refine，之后再进行100次常规epoch已达到实验结果。
这里的初始化 `r_s` 没有用之前GNN中的entity的特征而使用了随机初始化，每次Refine的时候都重新初始化 `r_s` 以保证GNN是与特征无关的邻域聚合。

用一个`toy_model`来测试一下：  
如果`S_hat`是一个`([1,  20,  10])`的tensor，于是`S`也是同维tensor.
```python
tensor([[[0.2081, 0.1068, 0.1042, 0.0795, 0.0790, 0.0654, 0.2081, 0.0312,
          0.0795, 0.0382],
         [0.1412, 0.1228, 0.1155, 0.0936, 0.0866, 0.1412, 0.0847, 0.0781,
          0.0562, 0.0800],
         [0.1831, 0.1236, 0.1213, 0.1211, 0.1142, 0.0592, 0.0685, 0.0387,
          0.0490, 0.1211],
         [0.1672, 0.1352, 0.1142, 0.1067, 0.0982, 0.0746, 0.0681, 0.1067,
          0.0449, 0.0841],
         [0.2663, 0.1230, 0.1188, 0.1153, 0.1067, 0.0367, 0.0437, 0.1067,
          0.0367, 0.0461],
         [0.1847, 0.0971, 0.0956, 0.0905, 0.0900, 0.0427, 0.1847, 0.0971,
          0.0555, 0.0620],
         [0.2072, 0.1531, 0.1098, 0.1015, 0.0992, 0.0936, 0.0864, 0.0404,
          0.0661, 0.0428],
         [0.1713, 0.1177, 0.1106, 0.1030, 0.1027, 0.0434, 0.0929, 0.1030,
          0.0769, 0.0783],
         [0.1444, 0.1065, 0.1032, 0.1001, 0.0827, 0.0827, 0.1444, 0.0681,
          0.0676, 0.1001],
         [0.2106, 0.1201, 0.1025, 0.1008, 0.0975, 0.0756, 0.0590, 0.1008,
          0.0756, 0.0573],
         [0.1446, 0.1196, 0.1168, 0.1119, 0.0982, 0.0906, 0.0482, 0.1168,
          0.0929, 0.0602],
         [0.1371, 0.1235, 0.1185, 0.1080, 0.0866, 0.0562, 0.1235, 0.0693,
          0.1080, 0.0693],
         [0.1632, 0.1443, 0.1270, 0.1119, 0.0937, 0.0508, 0.0552, 0.0683,
          0.0738, 0.1119],
         [0.1344, 0.1166, 0.1016, 0.1015, 0.0961, 0.1166, 0.1344, 0.0597,
          0.0961, 0.0430],
         [0.1730, 0.1338, 0.1172, 0.1102, 0.1101, 0.0711, 0.0673, 0.0398,
          0.1102, 0.0673],
         [0.1419, 0.1274, 0.1176, 0.1171, 0.1057, 0.1274, 0.0806, 0.0579,
          0.0421, 0.0823],
         [0.1284, 0.1235, 0.1211, 0.1138, 0.1121, 0.0873, 0.0737, 0.0889,
          0.0393, 0.1121],
         [0.2151, 0.1295, 0.0983, 0.0976, 0.0926, 0.0507, 0.0882, 0.0415,
          0.0983, 0.0882],
         [0.1548, 0.1119, 0.1018, 0.0945, 0.0923, 0.0645, 0.1548, 0.0232,
          0.1548, 0.0474],
         [0.1776, 0.1048, 0.1014, 0.0933, 0.0873, 0.1048, 0.0735, 0.1776,
          0.0345, 0.0451]]])
```
这里的20对应DGMC中的source KG的entity数量19388，10对应k=20.
`r_s`的维度是`([1, 20, 3])`，是随机的source KG的entity的embedding.：
```python
tensor([[[-0.4275, -0.5391,  1.3303],
         [-0.1886, -0.2301, -2.5228],
         [ 0.5658, -0.3961,  0.2925],
         [ 1.7480,  1.7999, -1.6263],
         [-1.0912,  1.0002, -0.6793],
         [-0.6771, -1.9398,  1.0686],
         [-1.3564, -2.2302, -1.1373],
         [-0.7566, -0.8479, -0.1599],
         [-1.2311,  0.4642, -1.0219],
         [ 0.2920,  1.5398,  0.2800],
         [-0.8212, -0.9775, -0.0262],
         [ 1.4581, -0.2900,  0.4935],
         [-0.6467, -0.2689, -0.7510],
         [ 0.2785,  1.5003, -0.9068],
         [ 0.7862, -0.5659, -0.7321],
         [ 1.1458, -0.5941,  1.0476],
         [ 0.0767,  0.5879, -1.2828],
         [ 1.1056, -0.7340,  0.7802],
         [ 1.6273,  0.3086, -0.8658],
         [ 0.9199, -0.3406, -0.4894]]])
```

`tmp_t = r_s.view(B, N_s, 1, R_in) * S.view(B, N_s, k, 1)` 中，`r_s.view(B, N_s, 1, R_in) `维度为`([1, 20, 1, 3])`， `S.view(B, N_s, k, 1)`维度为`([1, 20, 10, 1])`，这两个tensor做点乘，维度为`([1, 20, 10, 3])`：
```python
tensor([[[[-0.0890, -0.1122,  0.2768],
          [-0.0457, -0.0576,  0.1421],
          [-0.0446, -0.0562,  0.1387],
          [-0.0340, -0.0429,  0.1057],
          [-0.0338, -0.0426,  0.1051],
          [-0.0280, -0.0353,  0.0870],
          [-0.0890, -0.1122,  0.2768],
          [-0.0133, -0.0168,  0.0415],
          [-0.0340, -0.0429,  0.1057],
          [-0.0163, -0.0206,  0.0508]],

         [[-0.0266, -0.0325, -0.3562],
          [-0.0232, -0.0283, -0.3098],
          [-0.0218, -0.0266, -0.2915],
          [-0.0177, -0.0215, -0.2361],
          [-0.0163, -0.0199, -0.2184],
          [-0.0266, -0.0325, -0.3562],
          [-0.0160, -0.0195, -0.2137],
          [-0.0147, -0.0180, -0.1971],
          [-0.0106, -0.0129, -0.1418],
          [-0.0151, -0.0184, -0.2019]],

         [[ 0.1036, -0.0725,  0.0536],
          [ 0.0699, -0.0490,  0.0361],
          [ 0.0686, -0.0480,  0.0355],
          [ 0.0685, -0.0480,  0.0354],
          [ 0.0646, -0.0452,  0.0334],
          [ 0.0335, -0.0234,  0.0173],
          [ 0.0388, -0.0271,  0.0200],
          [ 0.0219, -0.0153,  0.0113],
          [ 0.0278, -0.0194,  0.0143],
          [ 0.0685, -0.0480,  0.0354]],

         [[ 0.2923,  0.3010, -0.2720],
          [ 0.2363,  0.2434, -0.2199],
          [ 0.1997,  0.2056, -0.1858],
          [ 0.1865,  0.1920, -0.1735],
          [ 0.1717,  0.1768, -0.1598],
          [ 0.1303,  0.1342, -0.1213],
          [ 0.1191,  0.1226, -0.1108],
          [ 0.1865,  0.1920, -0.1735],
          [ 0.0785,  0.0808, -0.0730],
          [ 0.1470,  0.1514, -0.1368]],

         [[-0.2906,  0.2664, -0.1809],
          [-0.1343,  0.1231, -0.0836],
          [-0.1297,  0.1188, -0.0807],
          [-0.1258,  0.1153, -0.0783],
          [-0.1164,  0.1067, -0.0725],
          [-0.0400,  0.0367, -0.0249],
          [-0.0477,  0.0437, -0.0297],
          [-0.1164,  0.1067, -0.0725],
          [-0.0400,  0.0367, -0.0249],tensor([[[[-0.0890, -0.1122,  0.2768],
          [-0.0457, -0.0576,  0.1421],
          [-0.0446, -0.0562,  0.1387],
          [-0.0340, -0.0429,  0.1057],
          [-0.0338, -0.0426,  0.1051],
          [-0.0280, -0.0353,  0.0870],
          [-0.0890, -0.1122,  0.2768],
          [-0.0133, -0.0168,  0.0415],
          [-0.0340, -0.0429,  0.1057],
          [-0.0163, -0.0206,  0.0508]],

         [[-0.0266, -0.0325, -0.3562],
          [-0.0232, -0.0283, -0.3098],
          [-0.0218, -0.0266, -0.2915],
          [-0.0177, -0.0215, -0.2361],
          [-0.0163, -0.0199, -0.2184],
          [-0.0266, -0.0325, -0.3562],
          [-0.0160, -0.0195, -0.2137],
          [-0.0147, -0.0180, -0.1971],
          [-0.0106, -0.0129, -0.1418],
          [-0.0151, -0.0184, -0.2019]],

         [[ 0.1036, -0.0725,  0.0536],
          [ 0.0699, -0.0490,  0.0361],
          [ 0.0686, -0.0480,  0.0355],
          [ 0.0685, -0.0480,  0.0354],
          [ 0.0646, -0.0452,  0.0334],
          [ 0.0335, -0.0234,  0.0173],
          [ 0.0388, -0.0271,  0.0200],
          [ 0.0219, -0.0153,  0.0113],
          [ 0.0278, -0.0194,  0.0143],
          [ 0.0685, -0.0480,  0.0354]],

         [[ 0.2923,  0.3010, -0.2720],
          [ 0.2363,  0.2434, -0.2199],
          [ 0.1997,  0.2056, -0.1858],
          [ 0.1865,  0.1920, -0.1735],
          [ 0.1717,  0.1768, -0.1598],
          [ 0.1303,  0.1342, -0.1213],
          [ 0.1191,  0.1226, -0.1108],
          [ 0.1865,  0.1920, -0.1735],
          [ 0.0785,  0.0808, -0.0730],
          [ 0.1470,  0.1514, -0.1368]],

         [[-0.2906,  0.2664, -0.1809],
          [-0.1343,  0.1231, -0.0836],
          [-0.1297,  0.1188, -0.0807],
          [-0.1258,  0.1153, -0.0783],
          [-0.1164,  0.1067, -0.0725],
          [-0.0400,  0.0367, -0.0249],
          [-0.0477,  0.0437, -0.0297],
          [-0.1164,  0.1067, -0.0725],
          [-0.0400,  0.0367, -0.0249],
          [-0.0503,  0.0461, -0.0313]],

         [[-0.1251, -0.3584,  0.1974],
          [-0.0658, -0.1884,  0.1038],
          [-0.0647, -0.1855,  0.1022],
          [-0.0613, -0.1755,  0.0967],
          [-0.0609, -0.1745,  0.0961],
          [-0.0289, -0.0828,  0.0456],
          [-0.1251, -0.3584,  0.1974],
          [-0.0658, -0.1884,  0.1038],
          [-0.0376, -0.1077,  0.0593],
          [-0.0420, -0.1203,  0.0663]],

         [[-0.2810, -0.4620, -0.2356],
          [-0.2076, -0.3413, -0.1741],
          [-0.1489, -0.2448, -0.1248],
          [-0.1377, -0.2264, -0.1155],
          [-0.1346, -0.2213, -0.1129],
          [-0.1270, -0.2087, -0.1065],
          [-0.1172, -0.1927, -0.0983],
          [-0.0548, -0.0901, -0.0459],
          [-0.0897, -0.1474, -0.0752],
          [-0.0580, -0.0954, -0.0486]],

         [[-0.1296, -0.1453, -0.0274],
          [-0.0890, -0.0998, -0.0188],
          [-0.0837, -0.0938, -0.0177],
          [-0.0779, -0.0873, -0.0165],
          [-0.0777, -0.0871, -0.0164],
          [-0.0329, -0.0368, -0.0069],
          [-0.0703, -0.0788, -0.0148],
          [-0.0779, -0.0873, -0.0165],
          [-0.0582, -0.0652, -0.0123],
          [-0.0593, -0.0664, -0.0125]],

         [[-0.1778,  0.0670, -0.1476],
          [-0.1311,  0.0494, -0.1089],
          [-0.1271,  0.0479, -0.1055],
          [-0.1232,  0.0465, -0.1023],
          [-0.1019,  0.0384, -0.0846],
          [-0.1019,  0.0384, -0.0846],
          [-0.1778,  0.0670, -0.1476],
          [-0.0838,  0.0316, -0.0695],
          [-0.0833,  0.0314, -0.0691],
          [-0.1232,  0.0465, -0.1023]],

         [[ 0.0615,  0.3243,  0.0590],
          [ 0.0351,  0.1850,  0.0336],
          [ 0.0299,  0.1578,  0.0287],
          [ 0.0294,  0.1552,  0.0282],
          [ 0.0285,  0.1502,  0.0273],
          [ 0.0221,  0.1165,  0.0212],
          [ 0.0172,  0.0909,  0.0165],
          [ 0.0294,  0.1552,  0.0282],
          [ 0.0221,  0.1165,  0.0212],
          [ 0.0167,  0.0882,  0.0160]],

         [[-0.1188, -0.1414, -0.0038],
          [-0.0982, -0.1169, -0.0031],
          [-0.0959, -0.1142, -0.0031],
          [-0.0919, -0.1094, -0.0029],
          [-0.0807, -0.0960, -0.0026],
          [-0.0744, -0.0886, -0.0024],
          [-0.0396, -0.0472, -0.0013],
          [-0.0959, -0.1142, -0.0031],
          [-0.0763, -0.0908, -0.0024],
          [-0.0495, -0.0589, -0.0016]],

         [[ 0.1999, -0.0397,  0.0676],
          [ 0.1801, -0.0358,  0.0610],
          [ 0.1729, -0.0344,  0.0585],
          [ 0.1574, -0.0313,  0.0533],
          [ 0.1263, -0.0251,  0.0428],
          [ 0.0820, -0.0163,  0.0278],
          [ 0.1801, -0.0358,  0.0610],
          [ 0.1010, -0.0201,  0.0342],
          [ 0.1574, -0.0313,  0.0533],
          [ 0.1010, -0.0201,  0.0342]],

         [[-0.1055, -0.0439, -0.1225],
          [-0.0933, -0.0388, -0.1084],
          [-0.0821, -0.0342, -0.0954],
          [-0.0724, -0.0301, -0.0840],
          [-0.0606, -0.0252, -0.0703],
          [-0.0328, -0.0137, -0.0381],
          [-0.0357, -0.0148, -0.0415],
          [-0.0441, -0.0184, -0.0513],
          [-0.0477, -0.0198, -0.0554],
          [-0.0724, -0.0301, -0.0840]],

         [[ 0.0374,  0.2017, -0.1219],
          [ 0.0325,  0.1749, -0.1057],
          [ 0.0283,  0.1525, -0.0921],
          [ 0.0283,  0.1522, -0.0920],
          [ 0.0268,  0.1442, -0.0872],
          [ 0.0325,  0.1749, -0.1057],
          [ 0.0374,  0.2017, -0.1219],
          [ 0.0166,  0.0895, -0.0541],
          [ 0.0268,  0.1442, -0.0872],
          [ 0.0120,  0.0645, -0.0390]],

         [[ 0.1360, -0.0979, -0.1267],
          [ 0.1052, -0.0757, -0.0979],
          [ 0.0922, -0.0663, -0.0858],
          [ 0.0867, -0.0624, -0.0807],
          [ 0.0865, -0.0623, -0.0806],
          [ 0.0559, -0.0402, -0.0521],
          [ 0.0529, -0.0381, -0.0492],
          [ 0.0313, -0.0225, -0.0292],
          [ 0.0867, -0.0624, -0.0807],
          [ 0.0529, -0.0381, -0.0492]],

         [[ 0.1626, -0.0843,  0.1486],
          [ 0.1460, -0.0757,  0.1335],
          [ 0.1348, -0.0699,  0.1232],
          [ 0.1342, -0.0696,  0.1227],
          [ 0.1212, -0.0628,  0.1108],
          [ 0.1460, -0.0757,  0.1335],
          [ 0.0923, -0.0479,  0.0844],
          [ 0.0663, -0.0344,  0.0606],
          [ 0.0482, -0.0250,  0.0441],
          [ 0.0942, -0.0489,  0.0862]],

         [[ 0.0098,  0.0755, -0.1647],
          [ 0.0095,  0.0726, -0.1584],
          [ 0.0093,  0.0712, -0.1553],
          [ 0.0087,  0.0669, -0.1460],
          [ 0.0086,  0.0659, -0.1437],
          [ 0.0067,  0.0513, -0.1119],
          [ 0.0056,  0.0433, -0.0945],
          [ 0.0068,  0.0523, -0.1140],
          [ 0.0030,  0.0231, -0.0504],
          [ 0.0086,  0.0659, -0.1437]],

         [[ 0.2378, -0.1579,  0.1678],
          [ 0.1432, -0.0950,  0.1010],
          [ 0.1087, -0.0721,  0.0767],
          [ 0.1079, -0.0716,  0.0761],
          [ 0.1024, -0.0680,  0.0723],
          [ 0.0561, -0.0372,  0.0396],
          [ 0.0975, -0.0647,  0.0688],
          [ 0.0459, -0.0305,  0.0324],
          [ 0.1087, -0.0721,  0.0767],
          [ 0.0975, -0.0647,  0.0688]],

         [[ 0.2519,  0.0478, -0.1340],
          [ 0.1821,  0.0345, -0.0969],
          [ 0.1657,  0.0314, -0.0882],
          [ 0.1538,  0.0292, -0.0818],
          [ 0.1502,  0.0285, -0.0799],
          [ 0.1050,  0.0199, -0.0558],
          [ 0.2519,  0.0478, -0.1340],
          [ 0.0378,  0.0072, -0.0201],
          [ 0.2519,  0.0478, -0.1340],
          [ 0.0771,  0.0146, -0.0410]],

         [[ 0.1634, -0.0605, -0.0869],
          [ 0.0964, -0.0357, -0.0513],
          [ 0.0933, -0.0346, -0.0496],
          [ 0.0859, -0.0318, -0.0457],
          [ 0.0804, -0.0298, -0.0427],
          [ 0.0964, -0.0357, -0.0513],
          [ 0.0677, -0.0251, -0.0360],
          [ 0.1634, -0.0605, -0.0869],
          [ 0.0317, -0.0117, -0.0169],
          [ 0.0415, -0.0154, -0.0221]]]])
          [-0.0503,  0.0461, -0.0313]],

         [[-0.1251, -0.3584,  0.1974],
          [-0.0658, -0.1884,  0.1038],
          [-0.0647, -0.1855,  0.1022],
          [-0.0613, -0.1755,  0.0967],
          [-0.0609, -0.1745,  0.0961],
          [-0.0289, -0.0828,  0.0456],
          [-0.1251, -0.3584,  0.1974],
          [-0.0658, -0.1884,  0.1038],
          [-0.0376, -0.1077,  0.0593],
          [-0.0420, -0.1203,  0.0663]],

         [[-0.2810, -0.4620, -0.2356],
          [-0.2076, -0.3413, -0.1741],
          [-0.1489, -0.2448, -0.1248],
          [-0.1377, -0.2264, -0.1155],
          [-0.1346, -0.2213, -0.1129],
          [-0.1270, -0.2087, -0.1065],
          [-0.1172, -0.1927, -0.0983],
          [-0.0548, -0.0901, -0.0459],
          [-0.0897, -0.1474, -0.0752],
          [-0.0580, -0.0954, -0.0486]],

         [[-0.1296, -0.1453, -0.0274],
          [-0.0890, -0.0998, -0.0188],
          [-0.0837, -0.0938, -0.0177],
          [-0.0779, -0.0873, -0.0165],
          [-0.0777, -0.0871, -0.0164],
          [-0.0329, -0.0368, -0.0069],
          [-0.0703, -0.0788, -0.0148],
          [-0.0779, -0.0873, -0.0165],
          [-0.0582, -0.0652, -0.0123],
          [-0.0593, -0.0664, -0.0125]],

         [[-0.1778,  0.0670, -0.1476],
          [-0.1311,  0.0494, -0.1089],
          [-0.1271,  0.0479, -0.1055],
          [-0.1232,  0.0465, -0.1023],
          [-0.1019,  0.0384, -0.0846],
          [-0.1019,  0.0384, -0.0846],
          [-0.1778,  0.0670, -0.1476],
          [-0.0838,  0.0316, -0.0695],
          [-0.0833,  0.0314, -0.0691],
          [-0.1232,  0.0465, -0.1023]],

         [[ 0.0615,  0.3243,  0.0590],
          [ 0.0351,  0.1850,  0.0336],
          [ 0.0299,  0.1578,  0.0287],
          [ 0.0294,  0.1552,  0.0282],
          [ 0.0285,  0.1502,  0.0273],
          [ 0.0221,  0.1165,  0.0212],
          [ 0.0172,  0.0909,  0.0165],
          [ 0.0294,  0.1552,  0.0282],
          [ 0.0221,  0.1165,  0.0212],
          [ 0.0167,  0.0882,  0.0160]],

         [[-0.1188, -0.1414, -0.0038],
          [-0.0982, -0.1169, -0.0031],
          [-0.0959, -0.1142, -0.0031],
          [-0.0919, -0.1094, -0.0029],
          [-0.0807, -0.0960, -0.0026],
          [-0.0744, -0.0886, -0.0024],
          [-0.0396, -0.0472, -0.0013],
          [-0.0959, -0.1142, -0.0031],
          [-0.0763, -0.0908, -0.0024],
          [-0.0495, -0.0589, -0.0016]],

         [[ 0.1999, -0.0397,  0.0676],
          [ 0.1801, -0.0358,  0.0610],
          [ 0.1729, -0.0344,  0.0585],
          [ 0.1574, -0.0313,  0.0533],
          [ 0.1263, -0.0251,  0.0428],
          [ 0.0820, -0.0163,  0.0278],
          [ 0.1801, -0.0358,  0.0610],
          [ 0.1010, -0.0201,  0.0342],
          [ 0.1574, -0.0313,  0.0533],
          [ 0.1010, -0.0201,  0.0342]],

         [[-0.1055, -0.0439, -0.1225],
          [-0.0933, -0.0388, -0.1084],
          [-0.0821, -0.0342, -0.0954],
          [-0.0724, -0.0301, -0.0840],
          [-0.0606, -0.0252, -0.0703],
          [-0.0328, -0.0137, -0.0381],
          [-0.0357, -0.0148, -0.0415],
          [-0.0441, -0.0184, -0.0513],
          [-0.0477, -0.0198, -0.0554],
          [-0.0724, -0.0301, -0.0840]],

         [[ 0.0374,  0.2017, -0.1219],
          [ 0.0325,  0.1749, -0.1057],
          [ 0.0283,  0.1525, -0.0921],
          [ 0.0283,  0.1522, -0.0920],
          [ 0.0268,  0.1442, -0.0872],
          [ 0.0325,  0.1749, -0.1057],
          [ 0.0374,  0.2017, -0.1219],
          [ 0.0166,  0.0895, -0.0541],
          [ 0.0268,  0.1442, -0.0872],
          [ 0.0120,  0.0645, -0.0390]],

         [[ 0.1360, -0.0979, -0.1267],
          [ 0.1052, -0.0757, -0.0979],
          [ 0.0922, -0.0663, -0.0858],
          [ 0.0867, -0.0624, -0.0807],
          [ 0.0865, -0.0623, -0.0806],
          [ 0.0559, -0.0402, -0.0521],
          [ 0.0529, -0.0381, -0.0492],
          [ 0.0313, -0.0225, -0.0292],
          [ 0.0867, -0.0624, -0.0807],
          [ 0.0529, -0.0381, -0.0492]],

         [[ 0.1626, -0.0843,  0.1486],
          [ 0.1460, -0.0757,  0.1335],
          [ 0.1348, -0.0699,  0.1232],
          [ 0.1342, -0.0696,  0.1227],
          [ 0.1212, -0.0628,  0.1108],
          [ 0.1460, -0.0757,  0.1335],
          [ 0.0923, -0.0479,  0.0844],
          [ 0.0663, -0.0344,  0.0606],
          [ 0.0482, -0.0250,  0.0441],
          [ 0.0942, -0.0489,  0.0862]],

         [[ 0.0098,  0.0755, -0.1647],
          [ 0.0095,  0.0726, -0.1584],
          [ 0.0093,  0.0712, -0.1553],
          [ 0.0087,  0.0669, -0.1460],
          [ 0.0086,  0.0659, -0.1437],
          [ 0.0067,  0.0513, -0.1119],
          [ 0.0056,  0.0433, -0.0945],
          [ 0.0068,  0.0523, -0.1140],
          [ 0.0030,  0.0231, -0.0504],
          [ 0.0086,  0.0659, -0.1437]],

         [[ 0.2378, -0.1579,  0.1678],
          [ 0.1432, -0.0950,  0.1010],
          [ 0.1087, -0.0721,  0.0767],
          [ 0.1079, -0.0716,  0.0761],
          [ 0.1024, -0.0680,  0.0723],
          [ 0.0561, -0.0372,  0.0396],
          [ 0.0975, -0.0647,  0.0688],
          [ 0.0459, -0.0305,  0.0324],
          [ 0.1087, -0.0721,  0.0767],
          [ 0.0975, -0.0647,  0.0688]],

         [[ 0.2519,  0.0478, -0.1340],
          [ 0.1821,  0.0345, -0.0969],
          [ 0.1657,  0.0314, -0.0882],
          [ 0.1538,  0.0292, -0.0818],
          [ 0.1502,  0.0285, -0.0799],
          [ 0.1050,  0.0199, -0.0558],
          [ 0.2519,  0.0478, -0.1340],
          [ 0.0378,  0.0072, -0.0201],
          [ 0.2519,  0.0478, -0.1340],
          [ 0.0771,  0.0146, -0.0410]],

         [[ 0.1634, -0.0605, -0.0869],
          [ 0.0964, -0.0357, -0.0513],
          [ 0.0933, -0.0346, -0.0496],
          [ 0.0859, -0.0318, -0.0457],
          [ 0.0804, -0.0298, -0.0427],
          [ 0.0964, -0.0357, -0.0513],
          [ 0.0677, -0.0251, -0.0360],
          [ 0.1634, -0.0605, -0.0869],
          [ 0.0317, -0.0117, -0.0169],
          [ 0.0415, -0.0154, -0.0221]]]])
```
`tmp_t`相当于source KG中每个entity的k个候选集embedding都乘以他们的**correspondences系数**。 
`idx`的维度为`([1, 20*10, 1])`，`tmp_t.view(B, N_s * k, R_in)`的维度为`([1, 20*10,  3]`。按照“代码库”里的介绍，`r_t = scatter_add(tmp_t, idx, dim=1, dim_size=N_t)`，按照`dim=1`维进行`scatter_add`，最终维度应该为`([1, x,  3]`，`x`为`index`索引最大的那个。<font color="red">我不太能理解就是为什么`x`能够正好和target KG entity的数量一样</font>。  
也就是说，`r_t`把`tmp_t`中那些在target KG中的entity的embeddings按照在`S`中得到的**correspondences系数**加和起来。

后面就是借助第二个GNN求出新的embedding，然后计算距离D.
后面的部分就不复杂了，先不介绍了（2020-10-23 10:52:19）